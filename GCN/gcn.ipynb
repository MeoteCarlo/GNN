{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi-layer Graph Convolutional Network (GCN) with first-order filters,来源:http://tkipf.github.io/graph-convolutional-networks/](images/gcn_web.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集-Cora Dataset\n",
    "\n",
    "## 下载地址\n",
    "\n",
    "https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "\n",
    "## 内容介绍\n",
    "\n",
    "Cora Dataset是对Machine Learning Paper进行分类的数据集，它包含三个文件:\n",
    "\n",
    "-- README: 对数据集的介绍;\n",
    "\n",
    "-- cora.cites: 论文之间的引用关系图。文件中每行包含两个Paper ID， 第一个ID是被引用的Paper ID； 第二个是引用的Paper ID。格式如下: <ID of cited paper> <ID of citing paper>\n",
    "\n",
    "-- cora.content: 包含了2708篇Paper的信息，每行的数据格式如下: <paper_id> <word_attributes>+ <class_label>。paper id是论文的唯一标识；word_attributes是是一个维度为1433的词向量，词向量的每个元素对应一个词，0表示该元素对应的词不在Paper中，1表示该元素对应的词在Paper中。class_label是论文的类别，每篇Paper被映射到如下7个分类之一: Case_Based、Genetic_Algorithms、Neural_Networks、Probabilistic_Methods、Reinforcement_Learning、Rule_Learning、Theory。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "   1     2     3     4     5     6     7     8     9     10    ...  1424  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "5     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "6     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "7     0     0     0     1     0     0     0     0     0     0  ...     0   \n",
      "8     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "9     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1425  1426  1427  1428  1429  1430  1431  1432  1433  \n",
      "0     0     0     1     0     0     0     0     0     0  \n",
      "1     0     1     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0  \n",
      "3     0     0     0     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0  \n",
      "5     0     0     1     0     0     0     0     0     0  \n",
      "6     0     0     0     0     0     0     0     0     0  \n",
      "7     0     0     0     0     0     0     0     0     0  \n",
      "8     0     0     0     0     0     0     0     0     0  \n",
      "9     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[10 rows x 1433 columns]\n",
      "labels:\n",
      "   Neural_Networks  Probabilistic_Methods  Reinforcement_Learning  \\\n",
      "0                1                      0                       0   \n",
      "1                0                      0                       0   \n",
      "2                0                      0                       1   \n",
      "3                0                      0                       1   \n",
      "4                0                      1                       0   \n",
      "5                0                      1                       0   \n",
      "6                0                      0                       0   \n",
      "7                1                      0                       0   \n",
      "8                1                      0                       0   \n",
      "9                0                      0                       0   \n",
      "\n",
      "   Rule_Learning  Theory  \n",
      "0              0       0  \n",
      "1              1       0  \n",
      "2              0       0  \n",
      "3              0       0  \n",
      "4              0       0  \n",
      "5              0       0  \n",
      "6              0       1  \n",
      "7              0       0  \n",
      "8              0       0  \n",
      "9              0       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 导入数据：分隔符为空格\n",
    "raw_data = pd.read_csv('data/cora/cora.content',sep = '\\t',header = None)\n",
    "num = raw_data.shape[0] # 样本点数2708\n",
    "\n",
    "\n",
    "raw_data_sample = raw_data.head(10)\n",
    "\n",
    "features =raw_data_sample.iloc[:,1:-1]\n",
    "\n",
    "labels = pd.get_dummies(raw_data_sample.iloc[:, -1])\n",
    "\n",
    "\n",
    "print(\"features:\")\n",
    "print(features)\n",
    "\n",
    "print(\"labels:\")\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0        1\n",
      "0  35     1033\n",
      "1  35   103482\n",
      "2  35   103515\n",
      "3  35  1050679\n",
      "4  35  1103960\n",
      "5  35  1103985\n",
      "6  35  1109199\n",
      "7  35  1112911\n",
      "8  35  1113438\n",
      "9  35  1113831\n"
     ]
    }
   ],
   "source": [
    "raw_data_cites = pd.read_csv('data/cora/cora.cites',sep = '\\t',header = None)\n",
    "\n",
    "print(raw_data_cites.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n",
      "features shape: (2708, 1433), labels shape: (2708, 7), adj shape: (2708, 2708)\n",
      "  (0, 8)\t1.0\n",
      "  (0, 14)\t1.0\n",
      "  (0, 258)\t1.0\n",
      "  (0, 435)\t1.0\n",
      "  (0, 544)\t1.0\n",
      "  (1, 344)\t1.0\n",
      "  (2, 410)\t1.0\n",
      "  (2, 471)\t1.0\n",
      "  (2, 552)\t1.0\n",
      "  (2, 565)\t1.0\n",
      "  (3, 197)\t1.0\n",
      "  (3, 463)\t1.0\n",
      "  (3, 601)\t1.0\n",
      "  (4, 170)\t1.0\n",
      "  (5, 490)\t1.0\n",
      "  (5, 2164)\t1.0\n",
      "  (6, 251)\t1.0\n",
      "  (6, 490)\t1.0\n",
      "  (7, 258)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (8, 14)\t1.0\n",
      "  (8, 258)\t1.0\n",
      "  (8, 435)\t1.0\n",
      "  (8, 751)\t1.0\n",
      "  (9, 308)\t1.0\n",
      "  :\t:\n",
      "  (2698, 2697)\t1.0\n",
      "  (2698, 2700)\t1.0\n",
      "  (2699, 2153)\t1.0\n",
      "  (2700, 2697)\t1.0\n",
      "  (2700, 2698)\t1.0\n",
      "  (2701, 2247)\t1.0\n",
      "  (2701, 2263)\t1.0\n",
      "  (2702, 881)\t1.0\n",
      "  (2702, 2624)\t1.0\n",
      "  (2703, 1221)\t1.0\n",
      "  (2703, 1409)\t1.0\n",
      "  (2703, 2200)\t1.0\n",
      "  (2704, 209)\t1.0\n",
      "  (2704, 2407)\t1.0\n",
      "  (2705, 1784)\t1.0\n",
      "  (2705, 1839)\t1.0\n",
      "  (2705, 1840)\t1.0\n",
      "  (2705, 2216)\t1.0\n",
      "  (2706, 1046)\t1.0\n",
      "  (2706, 1138)\t1.0\n",
      "  (2706, 1640)\t1.0\n",
      "  (2706, 1752)\t1.0\n",
      "  (2707, 774)\t1.0\n",
      "  (2707, 1389)\t1.0\n",
      "  (2707, 2344)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from util import load_data\n",
    "\n",
    "features, adj, labels = load_data()\n",
    "\n",
    "print('features shape: {}, labels shape: {}, adj shape: {}'.format(features.shape, labels.shape, adj.shape))\n",
    "\n",
    "print(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆分训练集、测试集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n",
      "[ True  True  True ... False False False]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "  (0, 0)\t0.16666666666666666\n",
      "  (0, 8)\t0.16666666666666666\n",
      "  (0, 14)\t0.09128709291752768\n",
      "  (0, 258)\t0.11785113019775792\n",
      "  (0, 435)\t0.16666666666666666\n",
      "  (0, 544)\t0.18257418583505536\n",
      "  (1, 1)\t0.5000000000000001\n",
      "  (1, 344)\t0.12500000000000003\n",
      "  (2, 2)\t0.19999999999999998\n",
      "  (2, 410)\t0.18257418583505536\n",
      "  (2, 471)\t0.15811388300841897\n",
      "  (2, 552)\t0.06819943394704735\n",
      "  (2, 565)\t0.05031546054266276\n",
      "  (3, 3)\t0.25\n",
      "  (3, 197)\t0.22360679774997896\n",
      "  (3, 463)\t0.2041241452319315\n",
      "  (3, 601)\t0.2041241452319315\n",
      "  (4, 4)\t0.5000000000000001\n",
      "  (4, 170)\t0.408248290463863\n",
      "  (5, 5)\t0.3333333333333333\n",
      "  (5, 490)\t0.2041241452319315\n",
      "  (5, 2164)\t0.23570226039551584\n",
      "  (6, 6)\t0.3333333333333333\n",
      "  (6, 251)\t0.19245008972987523\n",
      "  (6, 490)\t0.2041241452319315\n",
      "  :\t:\n",
      "  (2701, 2701)\t0.3333333333333333\n",
      "  (2702, 881)\t0.14907119849998599\n",
      "  (2702, 2624)\t0.2041241452319315\n",
      "  (2702, 2702)\t0.3333333333333333\n",
      "  (2703, 1221)\t0.25\n",
      "  (2703, 1409)\t0.1767766952966369\n",
      "  (2703, 2200)\t0.22360679774997896\n",
      "  (2703, 2703)\t0.25\n",
      "  (2704, 209)\t0.28867513459481287\n",
      "  (2704, 2407)\t0.3333333333333333\n",
      "  (2704, 2704)\t0.3333333333333333\n",
      "  (2705, 1784)\t0.14907119849998596\n",
      "  (2705, 1839)\t0.22360679774997896\n",
      "  (2705, 1840)\t0.2581988897471611\n",
      "  (2705, 2216)\t0.22360679774997896\n",
      "  (2705, 2705)\t0.19999999999999998\n",
      "  (2706, 1046)\t0.18257418583505536\n",
      "  (2706, 1138)\t0.16903085094570333\n",
      "  (2706, 1640)\t0.16903085094570333\n",
      "  (2706, 1752)\t0.31622776601683794\n",
      "  (2706, 2706)\t0.19999999999999998\n",
      "  (2707, 774)\t0.25\n",
      "  (2707, 1389)\t0.22360679774997896\n",
      "  (2707, 2344)\t0.3535533905932738\n",
      "  (2707, 2707)\t0.25\n"
     ]
    }
   ],
   "source": [
    "from util import get_splits, preprocess_adj\n",
    "\n",
    "DATASET = 'cora'\n",
    "\n",
    "X, A, y = load_data(dataset=DATASET)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)\n",
    "\n",
    "print(train_mask)\n",
    "\n",
    "\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "print(X)\n",
    "\n",
    "A_ = preprocess_adj(A, True)\n",
    "\n",
    "print(A_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tkipf/keras-gcn\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class GraphConvolution(tf.keras.layers.Layer):\n",
    "    \"\"\"Basic graph convolution layer as in https://arxiv.org/abs/1609.02907\"\"\"\n",
    "    def __init__(self, units, support=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                ):\n",
    "        \n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.support = support\n",
    "        assert support >= 1\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        features_shape = input_shapes[0]\n",
    "        assert len(features_shape) == 2\n",
    "        input_dim = features_shape[1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape = (input_dim * self.support, self.units),\n",
    "                                      initializer = self.kernel_initializer,\n",
    "                                      name = 'kernel',\n",
    "                                      regularizer = self.kernel_regularizer)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                       regularizer = self.kernel_regularizer)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        features = inputs[0]\n",
    "        basis = inputs[1:]\n",
    "\n",
    "        supports = list()\n",
    "        for i in range(self.support):\n",
    "            supports.append(K.dot(basis[i], features))\n",
    "        supports = K.concatenate(supports, axis=1)\n",
    "        output = K.dot(supports, self.kernel)\n",
    "\n",
    "        if self.bias:\n",
    "            output += self.bias\n",
    "            \n",
    "        return self.activation(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-94797f8d8975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf.keras.layers import Input, Dropout\n",
    "from tf.keras.models import Model\n",
    "from tf.keras.optimizers import Adam\n",
    "from tf.keras.regularizers import l2\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import time\n",
    "\n",
    "# Define parameters\n",
    "DATASET = 'cora'\n",
    "FILTER = 'localpool'  # 'chebyshev'\n",
    "MAX_DEGREE = 2  # maximum polynomial degree\n",
    "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "\n",
    "if FILTER == 'localpool':\n",
    "    \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
    "    print('Using local pooling filters...')\n",
    "    A_ = preprocess_adj(A, SYM_NORM)\n",
    "    support = 1\n",
    "    graph = [X, A_]\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n",
    "\n",
    "elif FILTER == 'chebyshev':\n",
    "    \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n",
    "    print('Using Chebyshev polynomial basis filters...')\n",
    "    L = normalized_laplacian(A, SYM_NORM)\n",
    "    L_scaled = rescale_laplacian(L)\n",
    "    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n",
    "    support = MAX_DEGREE + 1\n",
    "    graph = [X]+T_k\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(support)]\n",
    "\n",
    "else:\n",
    "    raise Exception('Invalid filter type.')\n",
    "\n",
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "\n",
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
